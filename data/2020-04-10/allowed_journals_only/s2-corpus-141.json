{"fieldsOfStudy":["Medicine","Mathematics"],"year":2006,"outCitations":[],"journalName":"Pharmaceutical statistics","paperAbstract":"","inCitations":["a837be31b6f1a6464168b0b6d801b181b51e140d"],"title":"Requiring 'independent' statistical analyses for industry sponsored trials?","doi":"10.1002/pst.195"}
{"fieldsOfStudy":["Computer Science","Medicine"],"year":2014,"outCitations":[],"journalName":"Journal of computational and graphical statistics : a joint publication of American Statistical Association, Institute of Mathematical Statistics, Interface Foundation of North America","paperAbstract":"The support vector machine (SVM) is a popular learning method for binary classification. Standard SVMs treat all the data points equally, but in some practical problems it is more natural to assign different weights to observations from different classes. This leads to a broader class of learning, the so-called weighted SVMs (WSVMs), and one of their important applications is to estimate class probabilities besides learning the classification boundary. There are two parameters associated with the WSVM optimization problem: one is the regularization parameter and the other is the weight parameter. In this paper we first establish that the WSVM solutions are jointly piecewise-linear with respect to both the regularization and weight parameter. We then develop a state-of-the-art algorithm that can compute the entire trajectory of the WSVM solutions for every pair of the regularization parameter and the weight parameter, at a feasible computational cost. The derived two-dimensional solution surface provides theoretical insight on the behavior of the WSVM solutions. Numerically, the algorithm can greatly facilitate the implementation of the WSVM and automate the selection process of the optimal regularization parameter. We illustrate the new algorithm on various examples.","inCitations":["6b1d190125c947c7130008f5361e8c81d49fb16f"],"title":"Two-Dimensional Solution Surface for Weighted Support Vector Machines.","doi":"10.1080/10618600.2012.761139"}
{"fieldsOfStudy":["Mathematics","Medicine"],"year":2012,"outCitations":[],"journalName":"Scandinavian journal of statistics, theory and applications","paperAbstract":"An objective of randomized placebo-controlled preventive HIV vaccine efficacy trials is to assess the relationship between the vaccine effect to prevent infection and the genetic distance of the exposing HIV to the HIV strain represented in the vaccine construct. Motivated by this objective, recently a mark-specific proportional hazards model with a continuum of competing risks has been studied, where the genetic distance of the transmitting strain is the continuous `mark' defined and observable only in failures. A high percentage of genetic marks of interest may be missing for a variety of reasons, predominantly due to rapid evolution of HIV sequences after transmission before a blood sample is drawn from which HIV sequences are measured. This research investigates the stratified mark-specific proportional hazards model with missing marks where the baseline functions may vary with strata. We develop two consistent estimation approaches, the first based on the inverse probability weighted complete-case (IPW) technique, and the second based on augmenting the IPW estimator by incorporating auxiliary information predictive of the mark. We investigate the asymptotic properties and finite-sample performance of the two estimators, and show that the augmented IPW estimator, which satisfies a double robustness property, is more efficient.","inCitations":["f46e3cc99ce205191034702bc1e50f7c48f366f5","d2f00f7cdee44fac0b9f6c37d0e063869a7baa82","314c0db5bbef99b5ff52d4f9ab46640032b3f714","13ff04d2d35bac954d1e44cd7c4ee0250866a1de","7a662b682e3ce27043e82992300ff7132d199f6a","9136b2638ddaadc9efc67dcf4c0c12f64bfc5283","6fc7a2f3864f8796cd3bfafd22fba0ff8797cbe1","cb47df4d7c4be66dd424c8ed00ea71705956ef26","3b8775433bf6c5d8f53fc66ca1830f34a6b6547b","5e744256603aa868810d6d341bf6fc705b835521"],"title":"Estimation of Stratified Mark-Specific Proportional Hazards Models with Missing Marks.","doi":"10.1111/j.1467-9469.2011.00746.x"}
{"fieldsOfStudy":["Medicine","Mathematics"],"year":2013,"outCitations":["0a14824290453b95d051ec9cc299d0f61ad82b23","0cba55f550d18630e6c4d4f3d78f203d4c238599","d50b597c475e87e03e8630e381011cb46e460ad8","7aa39f7f3b69473705e247dd2b3a9689f10fbbc3","db69e32846a9e4e80e1b58cb930a0d2017803300","bfc871e0e727433cb9ad5c90bccc9291ae246f61","5a343015d755a9001a26287b7a92028b5d026617","b365b8e45b7d81f081de44ac8f9eadf9144f3ca5","c9f5723859a665c3e54b8d9a1a7eecb8b5084af6"],"journalName":"Biostatistics","paperAbstract":"The PAF for an exposure is the fraction of disease cases in a population that can be attributed to that exposure. One method of estimating the PAF involves estimating the probability of having the disease given the exposure and confounding variables. In many settings, the exposure will interact with the confounders and the confounders will interact with each other. Also, in many settings, the probability of having the disease is thought, based on subject matter knowledge, to be a monotone increasing function of the exposure and possibly of some of the confounders. We develop an efficient approach for estimating logistic regression models with interactions and monotonicity constraints, and apply this approach to estimating the population attributable fraction (PAF). Our approach produces substantially more accurate estimates of the PAF in some settings than the usual approach which uses logistic regression without monotonicity constraints.","inCitations":["e2a20ce7184bc49a5c8115e16ea3145730b044c5","d8ff3acf7f8da05e00bb88159e813e20f0ac0a4b"],"title":"Efficient estimation of the attributable fraction when there are monotonicity constraints and interactions.","doi":"10.1093/biostatistics/kxs019"}
{"fieldsOfStudy":["Mathematics","Medicine"],"year":2010,"outCitations":["65bb01b985035307f7b1102e17b8a5c0f2dafff8","27339c19a18f910546897bd033169c648fa42600","f6a13f116e270dde9d67848495f801cdb8efa25d","261693c0671a0c52bbef78658e4a1b072ae89c1b","d0f3bbe7b5654427246bee62650f8f89ec3ba26c","c93dc7eeeee984d548e0a7173b43ad4ad43b3840","e7f9254f8e9678dedd8c2ccee99624c271ceeb14","dd5061631a4d11fa394f4421700ebf7e78dcbc59","41690226a4204d502781bb9f19743d73f4cfde8e","ac0f17e521c18ac0121d13487f625ff8ddc08eee","79148b9eaa267f72d201e1ac66cc3d9eb62ff112","bdfb57141b2141095ed942b28be24808aeba8d54","a9e0444b694a804a9088a622a6123e10a04430ae","a476a2b0b0debec5632f68406277ada18b21d195","f943acadcaa4ef8659a9bd373831be1d65f85003","33f54c9f71ebafd70db7cc6b9c4f311ac2ec916a","c31ddc5ba6213ee65c98eae3dba40293fb05d437","c63d59f6e684f7d0e3ed031668e3eb02fe2e375e","dd15652889b0025cf79c26e07bd2cebbc4282dbd","c499d177501f54bf2778dfa93fa58f5700535a62","00baab2c6a5a2dfe4883001d5901e373b48bd8ca","e0dc01248f2e19124afaac8f62e3ed2d935d652d","038742d1ccd731e65f9bbd112b4da91b50146643"],"journalName":"Scandinavian journal of statistics, theory and applications","paperAbstract":"Maximum likelihood estimation in many classical statistical problems is beset by multimodality. This article explores several variations of deterministic annealing that tend to avoid inferior modes and find the dominant mode. In Bayesian settings, annealing can be tailored to find the dominant mode of the log posterior. Our annealing algorithms involve essentially trivial changes to existing optimization algorithms built on block relaxation or the EM or MM principle. Our examples include estimation with the multivariate t distribution, Gaussian mixture models, latent class analysis, factor analysis, multidimensional scaling and a one-way random effects model. In the numerical examples explored, the proposed annealing strategies significantly improve the chances for locating the global maximum.","inCitations":["3963c444a51d94111e15c2e302f007d0544dc48c","fdffff8c1bf5cdd258f287136e0bbcd8ab0b7529","3015485b6b5ea13599184c103365a2ab641ceac4","02dd46ae960ba29b945fa2265cb37fa09282c469","d1b8e577420de4afee71346387eec169112ff38b","c057d6154c0dcb3d9885825d92c92fb0b9706b71","3244e0af2b787468b69624e7798a8c77b7222404","5b124e8dfd45951f322d3618c70684c118c1445b","c472d65a297e500d78440a7a28e5f0bc4c05527c","09075a03e97c6a12b79efb45bfbbe53f0fe7457c","9467e6b37c73f542610bdb7abda8520ae0217343","6fd3a2a1c7c1a9cb3a90a240b782a0b2d2385155","d7e84b1ceae70ab92581f1b9fede43500633a74e","1c432174dc233e1ba67fe78913a537651f5e1904","174fac646d527bd284a7aaef95234d0f9573f57a","b322d90a3b58ec69f41863b0e904c1614c2ccb9b","354cf7af969801dc84c376ae15cae51a1d53d632","8f147946de0bdc8bf75dcddf1aefa0977fccd8ac","6aee33f19d324b7e5e02df4ad77f995b6e98bee3","e0f0f5ff1e88386e74f45bf92a96d1d502a9b4f6"],"title":"On the Bumpy Road to the Dominant Mode.","doi":"10.1111/j.1467-9469.2009.00681.x"}
{"fieldsOfStudy":["Mathematics","Medicine"],"year":2017,"outCitations":[],"journalName":"The econometrics journal","paperAbstract":"We explore the asymptotic properties of strategic models of network formation in very large populations. Specifically, we focus on (undirected) exponential random graph models. We want to recover a set of parameters from the individuals' utility functions using the observation of a single, but large, social network. We show that, under some conditions, a simple logit-based estimator is coherent, consistent and asymptotically normally distributed under a weak version of homophily. The approach is compelling as the computing time is minimal and the estimator can be easily implemented using pre-programmed estimators available in most statistical packages. We provide an application of our method using the Add Health database.","inCitations":["07b13130eb8da72fe1638db35bb5278690630323","57f4f0d2faed9db605ce41dcb4c8d9235bb9aa69","6717fdc2ce4da473dc06c90663f88c1dbc6372a6","e5f07dce15f1ad726373f782f14289210e3c2f3f"],"title":"My friend far, far away: a random field approach to exponential random graph models.","doi":"10.1111/ectj.12096"}
{"fieldsOfStudy":["Mathematics","Medicine"],"year":2009,"outCitations":["27b3f344a1817a97c18b46f0b68577e76e781a51","18aac199eb2e72ce5d3ad1f160f143175135c0fa","64e10b601c7f29fdf141f00e9b0ad2e8c6fc68b1","4e162ffc5d61a738d63c1739b27f940e85ac138f","544385a40ad2e50aaefc2f66c55e29e065f4d370","4b2b7ab35d4628875c94463b57e4e994e9202215","bf61dcefd62b6cf862ce738ef0c818ebccaf8b0a","df725c48228846670ef357b7a83af8148ac4e127","02682749b7a6932dd4924a944b29d70334ba6fb4","bc5c84a861be8568848398ff8eb46c2e35826db6","481063dc8aca04aa7028290be467f2ec2a34fbc3","1d2965f2ca21661e8ee7fc16035d3b905ddf2d40","a7e4923573a4de213c91a919432fc8db8a1d7166","72b4cf3034e57bb46bf31dc29955b6b9144fef31","64b4ddcf066597a200423c55652f11ce89780063","a59b6026f22adcc4688335d10461f36d67270534","17098ee2588fc050a886367333ff0bdfc151262e","8e09827a18fede2db3d6a9f1e932bc472de53c42","eea6ced83cd18ad9a7cd7f583acc709705ab36b4","ab4855e817351a2b71cfca7e40be5bb0f285f19d","4ccf4792e012fad2ef65c5bcc68e188ae9409312","49fdffffa7befc5ed28d9799a3b66a3e8d014c1f","a2f325838088daef56b29fe112d744663c959aa1","be90348fcd7e5043904e417575ec12cf07219ece","16bca73bb91f456c20d853b52b21d79e932e3b2e","fcf09a957061a10b84264e670ee9416cb27ba726"],"journalName":"Journal of the Royal Statistical Society. Series C, Applied statistics","paperAbstract":"Public health concerns over the occurrence of birth defects and developmental abnormalities that may occur as a result of prenatal exposure to drugs, chemicals, and other environmental factors has led to an increasing number of developmental toxicity studies. Because fetal pups are commonly evaluated for multiple outcomes, data analysis frequently involves a joint modeling approach. In this paper, we focus on modelling clustered binary and continuous outcomes in the setting where both outcomes are potentially observable in all offspring but, due to practical limitations, the continuous outcome is only observed in a subset of offspring. The subset is not a simple random sample (SRS) but is selected by the experimenter under a prespecified probability model.While joint models for binary and continuous outcomes have been developed when both outcomes are available for every fetus, many existing approaches are not directly applicable when the continuous outcome is not observed in a SRS. We adapt a likelihood-based approach for jointly modelling clustered binary and continuous outcomes when the continuous response is missing by design and missingness depends on the binary trait. The approach takes into account the probability that a fetus is selected in the subset. Through the use of a partial likelihood, valid estimates can be obtained by a simple modification to the partial likelihood score. Data involving the herbicide 2,4,5-T are analyzed. Simulation results confirm the approach.","inCitations":["0c48a94b471ae6442087ec7f6633056a4363c617","8985580b0d1a9fc5be6fe0baeec27a164aade7cf","a6367d1ab1dc0d1b9f02e6a1f184eb9f5eeb60fb"],"title":"A Novel Application of a Bivariate Regression Model for Binary and Continuous Outcomes to Studies of Fetal Toxicity.","doi":"10.1111/j.1467-9876.2009.00667.x"}
{"fieldsOfStudy":["Medicine","Mathematics"],"year":1968,"outCitations":[],"journalName":"The British journal of mathematical and statistical psychology","paperAbstract":"Two tones, T1 and T2, differing in loudness, were employed as stimuli in a discriminative binary prediction task. Two responses, A1 and A2, predicting events E1 and E2 respectively, were available to the subjects. An A1 response was correct with probability π1 and π2 on T1 and T2 trials, respectively. The present study assessed the effects of the covariation of π2 and the similarity of T1 and T2 on P(A1|T1), the probability of an A1 response given a T1 trial. Subjects were also required to identify the trial type (loud or soft) and were given partial feedback of identification responding. Partial feedback was sufficient to eliminate the effects of cue similarity on discriminative event prediction and also improved identification of trial types. The reinforcement effects indicated that subjects learn to behave appropriately to identified cues rather than learn to make specific motor responses.","inCitations":["cf4caaaf943d4079f01afbeb305ae3efc8554023"],"title":"Discriminative binary prediction with reinforced cue identification.","doi":"10.1111/j.2044-8317.1968.tb00409.x"}
{"fieldsOfStudy":["Mathematics","Medicine"],"year":2019,"outCitations":[],"journalName":"Electronic journal of statistics","paperAbstract":"Treatment rules based on individual patient characteristics that are easy to interpret and disseminate are important in clinical practice. Properly planned and conducted randomized clinical trials are used to construct individualized treatment rules. However, it is often a concern that trial participants lack representativeness, so it limits the applicability of the derived rules to a target population. In this work, we use data from a single trial study to propose a two-stage procedure to derive a robust and parsimonious rule to maximize the benefit in the target population. The procedure allows a wide range of possible covariate distributions in the target population, with minimal assumptions on the first two moments of the covariate distribution. The practical utility and favorable performance of the methodology are demonstrated using extensive simulations and a real data application.","inCitations":["4207142e36b657d3f18ffa74935947383d0eb8d4","6068f7a6c1c8f3f3dd9964482b6049d7932489e2"],"title":"Robustifying Trial-Derived Optimal Treatment Rules for A Target Population.","doi":"10.1214/19-EJS1540"}
{"fieldsOfStudy":["Medicine","Mathematics"],"year":2009,"outCitations":[],"journalName":"Journal of statistical planning and inference","paperAbstract":"A challenge for implementing performance based Bayesian sample size determination is selecting which of several methods to use. We compare three Bayesian sample size criteria: the average coverage criterion (ACC) which controls the coverage rate of fixed length credible intervals over the predictive distribution of the data, the average length criterion (ALC) which controls the length of credible intervals with a fixed coverage rate, and the worst outcome criterion (WOC) which ensures the desired coverage rate and interval length over all (or a subset of) possible datasets. For most models, the WOC produces the largest sample size among the three criteria, and sample sizes obtained by the ACC and the ALC are not the same. For Bayesian sample size determination for normal means and differences between normal means, we investigate, for the first time, the direction and magnitude of differences between the ACC and ALC sample sizes. For fixed hyperparameter values, we show that the difference of the ACC and ALC sample size depends on the nominal coverage, and not on the nominal interval length. There exists a threshold value of the nominal coverage level such that below the threshold the ALC sample size is larger than the ACC sample size, and above the threshold the ACC sample size is larger. Furthermore, the ACC sample size is more sensitive to changes in the nominal coverage. We also show that for fixed hyperparameter values, there exists an asymptotic constant ratio between the WOC sample size and the ALC (ACC) sample size. Simulation studies are conducted to show that similar relationships among the ACC, ALC, and WOC may hold for estimating binomial proportions. We provide a heuristic argument that the results can be generalized to a larger class of models.","inCitations":["da01982bd28a704f9f1c67e2c9bf71c2ea3a0cb4","be4c68f819f257c07a0a687e576cbafede91ee39","f3d25a239c531723def3811fcc06e56d4cadf6a8","140a9875f67156ce77fb391166108d0dc2920013","88a81da5e82359cab83a69c59a70b848858e3230"],"title":"Comparison of Bayesian Sample Size Criteria: ACC, ALC, and WOC.","doi":"10.1016/j.jspi.2009.05.041"}
{"fieldsOfStudy":["Medicine","Mathematics"],"year":2015,"outCitations":[],"journalName":"Pharmaceutical statistics","paperAbstract":"Proactive evaluation of drug safety with systematic screening and detection is critical to protect patients' safety and important in regulatory approval of new drug indications and postmarketing communications and label renewals. In recent years, quite a few statistical methodologies have been developed to better evaluate drug safety through the life cycle of the product development. The statistical methods for flagging safety signals have been developed in two major areas - one for data collected from spontaneous reporting system, mostly in the postmarketing area, and the other for data from clinical trials. To our knowledge, the methods developed for one area have not been applied to the other one so far. In this article, we propose to utilize all such methods for flagging safety signals in both areas regardless of which specific area they were originally developed for. Therefore, we selected eight typical methods, that is, proportional reporting ratios, reporting odds ratios, the maximum likelihood ratio test, Bayesian confidence propagation neural network method, chi-square test for rates comparison, Benjamini and Hochberg procedure, new double false discovery rate control procedure, and Bayesian hierarchical mixture model for systematic comparison through simulations. The Benjamini and Hochberg procedure and new double false discovery rate control procedure perform best overall in terms of sensitivity and false discovery rate. The likelihood ratio test also performs well when the sample sizes are large.","inCitations":["a4bffc60fce2def54e87eb6a106fdc6d0c0e8d09"],"title":"Evaluation of statistical methods for safety signal detection: a simulation study.","doi":"10.1002/pst.1652"}
{"fieldsOfStudy":["Medicine","Mathematics"],"year":2015,"outCitations":[],"journalName":"Journal of the Royal Statistical Society. Series C, Applied statistics","paperAbstract":"Hierarchical group testing is widely used to test individuals for diseases. This testing procedure works by first amalgamating individual specimens into groups for testing. Groups testing negatively have their members declared negative. Groups testing positively are subsequently divided into smaller subgroups and are then retested to search for positive individuals. In our paper, we propose a new class of informative retesting procedures for hierarchical group testing that acknowledges heterogeneity among individuals. These procedures identify the optimal number of groups and their sizes at each testing stage in order to minimize the expected number of tests. We apply our proposals in two settings: 1) HIV testing programs that currently use three-stage hierarchical testing and 2) chlamydia and gonorrhea screening practices that currently use individual testing. For both applications, we show that substantial savings can be realized by our new procedures.","inCitations":["75b1e0ef4fc4a0a96a9bd47132191232efda0525","3ac65a9698aedbb101f5974f698089d9f038b927","91c18781553482c7fb395f0aca9351a744555e7e","075e33d0e06d879369ef0c4d279b24ff0a36e084","b9b0e3c0fc8e5defb6e168fe8375c5849a030944","2af0b3f946bf540d69f03a39a02c3617bfc21160"],"title":"Optimal retesting configurations for hierarchical group testing.","doi":"10.1111/rssc.12097"}
{"fieldsOfStudy":["Medicine","Mathematics"],"year":2018,"outCitations":[],"journalName":"Journal of biopharmaceutical statistics","paperAbstract":"A crucial component of making individualized treatment decisions is to accurately predict each patient's disease risk. In clinical oncology, disease risks are often measured through time-to-event data, such as overall survival and progression/recurrence-free survival, and are often subject to censoring. Risk prediction models based on recursive partitioning methods are becoming increasingly popular largely due to their ability to handle nonlinear relationships, higher-order interactions, and/or high-dimensional covariates. The most popular recursive partitioning methods are versions of the Classification and Regression Tree (CART) algorithm, which builds a simple interpretable tree structured model. With the aim of increasing prediction accuracy, the random forest algorithm averages multiple CART trees, creating a flexible risk prediction model. Risk prediction models used in clinical oncology commonly use both traditional demographic and tumor pathological factors as well as high-dimensional genetic markers and treatment parameters from multimodality treatments. In this article, we describe the most commonly used extensions of the CART and random forest algorithms to right-censored outcomes. We focus on how they differ from the methods for noncensored outcomes, and how the different splitting rules and methods for cost-complexity pruning impact these algorithms. We demonstrate these algorithms by analyzing a randomized Phase III clinical trial of breast cancer. We also conduct Monte Carlo simulations to compare the prediction accuracy of survival forests with more commonly used regression models under various scenarios. These simulation studies aim to evaluate how sensitive the prediction accuracy is to the underlying model specifications, the choice of tuning parameters, and the degrees of missing covariates.","inCitations":["6d27c5c88161166c5fb407bf05d79ae48a6f3b98","f6761f750694b519f85a472db519afb1442b4e58","d266baa5808ed5a36fc27417414e9ebf6644afa1"],"title":"Personalized Risk Prediction in Clinical Oncology Research: Applications and Practical Issues Using Survival Trees and Random Forests.","doi":"10.1080/10543406.2017.1377730"}
{"fieldsOfStudy":["Computer Science","Medicine"],"year":2015,"outCitations":["07d76fee4969bb3398784f5883db31bf351fb667","963ffb00f873d7af7f2b47fdfff79829670c6d71","bdfb57141b2141095ed942b28be24808aeba8d54","8f583c23653b117fb8232b94a3d40a3e49c69656","f25fdf41cdedb2e4c646d3a862cfa3cf970efe68","8c1bd5331c7d1cb19574c2ce4470d96c9d8044a8","84fa734a26a63dc4476ae76fbb04f4955c4b3287","7dd2f7da6be15143836010bb600e9fcef76ac823","8a25ae38cab87409fdab754bfc7535c0f8ddb980","4da107f0fea4e956c89be005066e876eae564a56","f37f8cf2e68c44caffba1e43f48f8df02ccdfc74","9f86ca3f2bf428fe34332d817a3f3f4c27095d7f","a3017f91fdd344de3fab60232609ae1a9f8c560b","7e459946cb320935ee97eb9ffa23136524866257","2a6375a6f4bf66e50fcaa0b0e4b184d029ac927d","692dcd0882d9ad2168bdffc9ab6a31c9a90dd943","0ecfd2048fe0786ee706dde097e4eb6bf66b8aa8","b3e596c82d94117962ca4d2ba3fed81d6060d1c9","9ebb5c0d6d54707a4d6181a693b6f755ec8a45a9","b477dd12dd49e44a62c1a303501df5fb6706c7e9","c373f7ab5b7d1ac3f8fc69609968c981f6ff77e0","6e820cf11712b9041bb625634612a535476f0960","ff4b3bbb455c9cc561ddec097a869140b3c1303d","2b266061d820b7670384845819c6f6707f2f8fda","35083c29c8e7e717cab4207cf81b5f007ce5a6f4","3b754b458b125772fee0460957d69366d07bb864","7b7a347c706088a1e6afdf645d79016bcf62e59a"],"journalName":"Journal of computational and graphical statistics : a joint publication of American Statistical Association, Institute of Mathematical Statistics, Interface Foundation of North America","paperAbstract":"Principal component analysis (PCA) is a popular dimension reduction method to reduce the complexity and obtain the informative aspects of high-dimensional datasets. When the data distribution is skewed, data transformation is commonly used prior to applying PCA. Such transformation is usually obtained from previous studies, prior knowledge, or trial-and-error. In this work, we develop a model-based method that integrates data transformation in PCA and finds an appropriate data transformation using the maximum profile likelihood. Extensions of the method to handle functional data and missing values are also developed. Several numerical algorithms are provided for efficient computation. The proposed method is illustrated using simulated and real-world data examples.","inCitations":["5def0f6089560276f1f13ab52a5750e9f9e6092e","c39fb7c5a252e58618e1808f7d99765580f15ef0","d6faa861c9888f1ddd96c9b7c5aa050f3312ba87","8f1724edd2a03c0d986702e06fbf8efb5bc550d0"],"title":"Integrating Data Transformation in Principal Components Analysis.","doi":"10.1080/10618600.2014.891461"}
{"fieldsOfStudy":["Medicine","Mathematics"],"year":2015,"outCitations":["2ead934017486db0e1d812b462f91b4d441daaa8","843ffe7fbf1dc614ccf8ef3114773c405c4029e7","3b19bf6e1ed0cdfdbaf174ec657fea83f4380e11","52229759bfc813a4aed0a2b3b74fc90f7e821982","560e50ad293000d83727e3ed3f141677e13c5b36","c0b534e63061060ef1b266fc3f9237c58d31b1c9","811c89ba0b4388b4de1faa3d179fc577261a87f4","9a1a73d67042a1f60c18b84cb2f607a529bad2fe","c9f9004ca5b46dd2eaa6b310c9d42cd1c52017a1","d2e4c7113a59ebe6e46590eda8e5ce6858980b4a","823d470f417d0365556f7ed18a4b9ea36f2a73f4","ca5ffe77e14cea51e1a7af8e222504ce85de287f","af4efde3032fc045ac0501f3f2ac581096dc0bbf","bbdcccc879a38f818ca45c510fe74b574279bc64","a461e4911af4aca82591c17939a812d515e67908","92de17c6ab1b3e14dfd2ae75bb4dbd2bdaa9c91c","83e3b7e2503da317934a1e83ed585d004b661484","70bb497b966f9e7a2f08c19a8970764223abdf6d","ae1f00ae1d1a5155433c33ccbcf5a6b7bca98e84","3d5e8d3a9a24e149c6b7e87e4d452d470e8a73ec","55f4616cd4d53d6cb2f75c38e4243e0b30feec49","184a688f5bf5033b4e09fd0b9e19fe7d951a3af6","4ebdfaef8dcc8d31058340741ba6f74c703a90c3","4dfa8b06fe88377111706b7a630e74d8f559eeb3","42974a682e6792e1c53b2a7f393bca5d60073a04"],"journalName":"Biostatistics","paperAbstract":"Prospective pregnancy studies are a valuable source of longitudinal data on menstrual cycle length. However, care is needed when making inferences of such renewal processes. For example, accounting for the sampling plan is necessary for unbiased estimation of the menstrual cycle length distribution for the study population. If couples can enroll when they learn of the study as opposed to waiting for the start of a new menstrual cycle, then due to length-bias, the enrollment cycle will be stochastically larger than the general run of cycles, a typical property of prevalent cohort studies. Furthermore, the probability of enrollment can depend on the length of time since a woman's last menstrual period (a backward recurrence time), resulting in selection effects. We focus on accounting for length-bias and selection effects in the likelihood for enrollment menstrual cycle length, using a recursive two-stage approach wherein we first estimate the probability of enrollment as a function of the backward recurrence time and then use it in a likelihood with sampling weights that account for length-bias and selection effects. To broaden the applicability of our methods, we augment our model to incorporate a couple-specific random effect and time-independent covariate. A simulation study quantifies performance for two scenarios of enrollment probability when proper account is taken of sampling plan features. In addition, we estimate the probability of enrollment and the distribution of menstrual cycle length for the study population of the Longitudinal Investigation of Fertility and the Environment Study.","inCitations":["e3a2765b7bdf615aeced1adb06ebe585c60a19ac","3a49a9e0fa1733d25587abeff1e10cdb43457020","95e9210180a4567aac9141c16b9aeb050bab9ebd","a9de76fdd0e31a3116965fd5f83e14aa1840f99a","bb5cea5bb3f6b3f6a6e74efedb5eff795f83b84e","5861c3b93c403f3d6a7b1b13f1214c01df4044bc","f2f91bf5b65c40696adcb707768c5e97deba7472","4c148dc8c83fa2dd998a97b5f5597886beab6645"],"title":"Accounting for length-bias and selection effects in estimating the distribution of menstrual cycle length.","doi":"10.1093/biostatistics/kxu035"}
{"fieldsOfStudy":["Medicine","Mathematics"],"year":2013,"outCitations":["3a00a15335d31c5af9af6340d19b9f5968ada4c0","00391a69c4849fdd462042b6b088993a6ebd6f54","4cbb3afdc51bbf0131081cfe38e309e46b115d98","a7fee9072b5cc9dafbc9a4851cdd4c9ea66c5aa2","aa5308884ea8b58c97c1b385297dcbb7e45546e7","246107cf05f1de9601f8af81ebddd61846925220","c07be1c8fc2c0a98e95832dd704816264b33c9df","cf137793c52bea724fe6e7547c000b3dfa534780","088eaef46160b9ff29238efc5a9cc0966fef2004","bc69d0b5768eaf2b5f6eca5714319e7214e060b6","924d04027f7205fb56222bdbbc34a386f7837db3","d399ca703ed5d0b9a0ba62cbfa7383109c3957d0","63981a605b4f5c4792c39ba07860879a2d6110d8","1b6546689a627dc24a317ec49d383b90df42c615"],"journalName":"Pharmaceutical statistics","paperAbstract":"Proschan, Brittain, and Kammerman made a very interesting observation that for some examples of the unequal allocation minimization, the mean of the unconditional randomization distribution is shifted away from 0. Kuznetsova and Tymofyeyev linked this phenomenon to the variations in the allocation ratio from allocation to allocation in the examples considered in the paper by Proschan et al. and advocated the use of unequal allocation procedures that preserve the allocation ratio at every step. In this paper, we show that the shift phenomenon extends to very common settings: using conditional randomization test in a study with equal allocation. This phenomenon has the same cause: variations in the allocation ratio among the allocation sequences in the conditional reference set, not previously noted. We consider two kinds of conditional randomization tests. The first kind is the often used randomization test that conditions on the treatment group totals; we describe the variations in the conditional allocation ratio with this test on examples of permuted block randomization and biased coin randomization. The second kind is the randomization test proposed by Zheng and Zelen for a multicenter trial with permuted block central allocation that conditions on the within-center treatment totals. On the basis of the sequence of conditional allocation ratios, we derive the value of the shift in the conditional randomization distribution for specific vector of responses and the expected value of the shift when responses are independent identically distributed random variables. We discuss the asymptotic behavior of the shift for the two types of tests.","inCitations":["4cbb3afdc51bbf0131081cfe38e309e46b115d98"],"title":"Shift in re-randomization distribution with conditional randomization test.","doi":"10.1002/pst.1556"}
{"fieldsOfStudy":["Medicine","Mathematics"],"year":2014,"outCitations":["ab54df6d7b353f029ff885aaee1971b9e8af6f93","0150ed9114217b09d04bc95ea73014f5cf203530","b037e6b5c1b6132c2bb724cdc9011e5fcc176c35","007fc14b855afbc2929d71c37f3fb6e5f112263e","e2b848c59f6e6964bce9f1d8d2d621e47339b92b","ca807fa2d24db79de60427eb62a3dea2f31b2ae8","3af3a6f6daca9dbb3af142cc436b2b5f8bd1d5e0","7a6eddd5e7350867f86aa739d0d72a08d4388fbc","efe973075ac6c86965fdc937e44d5bbac34d00c9","e73f5b2077067d3c28aa7afde4639da7df161ad5","0b3bf78d63374eec590204de72d36aa75722278d","c457f99a41dcc64371f802f7c1ec6f5dc5207b4c"],"journalName":"Biostatistics","paperAbstract":"When there is evidence of long-term survivors, cure models are often used to model the survival curve. A cure model is a mixture model consisting of a cured fraction and an uncured fraction. Traditional cure models assume that the cured or uncured status in the censored set cannot be distinguished. But in many practices, some diagnostic procedures may provide partial information about the cured or uncured status relative to certain sensitivity and specificity. The traditional cure model does not take advantage of this additional information. Motivated by a clinical study on bone injury in pediatric patients, we propose a novel extension of a traditional Cox proportional hazards (PH) cure model that incorporates the additional information about the cured status. This extension can be applied when the latency part of the cure model is modeled by the Cox PH model. Extensive simulations demonstrated that the proposed extension provides more efficient and less biased estimations, and the higher efficiency and smaller bias is associated with higher sensitivity and specificity of diagnostic procedures. When the proposed extended Cox PH cure model was applied to the motivating example, there was a substantial improvement in the estimation.","inCitations":["d7d48d57a86a0a16b7e49abedcdc6b7db61c6b28","5968fe9a0365fb18f099a25def5a86aff905c4fd","da27f196c1a3deac414f58874742a39c51a1a926","21f123d05987ac681728159e4e3a0bc9a4c27b8e","73a74b500f3dd8e7f7edd3546828160ac3e772f7","82ab1214e5d888e31b5e1ad3fee2ec9815a9f31f"],"title":"Extension of a Cox proportional hazards cure model when cure information is partially known.","doi":"10.1093/biostatistics/kxu002"}
{"fieldsOfStudy":["Computer Science","Medicine"],"year":2013,"outCitations":[],"journalName":"Journal of computational and graphical statistics : a joint publication of American Statistical Association, Institute of Mathematical Statistics, Interface Foundation of North America","paperAbstract":"We propose a two-stage model selection procedure for the linear mixed-effects models. The procedure consists of two steps: First, penalized restricted log-likelihood is used to select the random effects, and this is done by adopting a Newton-type algorithm. Next, the penalized log-likelihood is used to select the fixed effects via pathwise coordinate optimization to improve the computation efficiency. We prove that our procedure has the oracle properties. Both simulation studies and a real data example are carried out to examine finite sample performance of the proposed fixed and random effects selection procedure. Supplementary materials including R code used in this article and proofs for the theorems are available online.","inCitations":["b30cad7bc48545f43f339ac67d8715ac3899d3d3","fa96e5d61cfba7173186a42828e764132eb811a7","d9d0cac5e99e1a22434db1922168bb89b3d12ee7","bf93f1c23f8630cccb69f38214d4524fe9eda3a3","252b752fbc8e3110f94115d80d5be9924736f644","9d7c347513b1b55d56a47ed57f9b81c9a3562cc8","321bdaff28deab928ab088146e840c61fe27625e"],"title":"Fixed and Random Effects Selection by REML and Pathwise Coordinate Optimization.","doi":"10.1080/10618600.2012.681219"}
{"fieldsOfStudy":["Mathematics","Medicine"],"year":2001,"outCitations":[],"journalName":"The British journal of mathematical and statistical psychology","paperAbstract":"A small proportion of outliers can distort the results based on classical procedures in covariance structure analysis. We look at the quantitative effect of outliers on estimators and test statistics based on normal theory maximum likelihood and the asymptotically distribution-free procedures. Even if a proposed structure is correct for the majority of the data in a sample, a small proportion of outliers leads to biased estimators and significant test statistics. An especially unfortunate consequence is that the power to reject a model can be made arbitrarily--but misleadingly--large by inclusion of outliers in an analysis.","inCitations":["25ca17a76b94c14f89284337425a0e839add812d","5e60c24ed2fbe95828ec33341ca36ce9b9772439","27e0e266c2c149d28ce22e7322713b8b3c68c636","d809e2b3cf893ea5a2dce0f1590d00b80732d575","828a8a2f415e25026a7c7d30324f8708016d3d10","b573c7cc9a69504e3fbe9ee3574dc645155080a3","4353e51af1c2d04b8063b5ed8d817380f7ab2298","4c604ed6b8f917f5de5e5ae38302327d2dc2c930","f5108200ee0d9ea02d51814f182c3fb193517ea6","9a2f07cb5272f9fddf692f6965b2cb1d020ef312","236f151ba111c689c58b2ad400f50199c7e08e29","9163772cfaefa46691d6760cbf745d22e91537de","07bfff26ff5ed410d353f7a26022cca65fc56b9d","ba47002d285f8b2a49f61416bd7b9486b2f4e907","012836b29f66479114c52eb2a0ce9c9033079a4d","73d53425c7f8ba5536f9596ca8e79b276fb6f55b","4e7ac9dec28c862b5de9d85bf54b4fd4ccbc0236","c12f57ff3c53ac4bb9ad4e83dce82beb2448343c","f790ee904a95b41297628d8049defbb80a3919c4","114a9caebe924eb95cd4cf1d7b6884810634fe04","b6b3fb95899c7cee0a9ec3d97bca0638cdaa59d0","0963e2cfca95040a57e96933ada3f9f8c2196ae6","1facb4bcd0ef692a37dbe79c79f12adeb91e90bd","56f7dd338ca6beb914a458a2de3583ef82e60d70","df40565c2fd3129899f600a42e77889f0410be0d","c91bd7599575882f55861121b990fe0fd8d7c2e3","b2e5a89b744bb9615791485bb436170aaea39709","e8ca4925461da00a00736d86b85564a0ad6832c4","18de534fdf4c996f31194c050fd4cf014dfa6732","ad59d62ebc58c50cc3fc0f0018ec3368dbd00306","faf63349439c31171633d6a975902ca8cd27c7fe"],"title":"Effect of outliers on estimators and tests in covariance structure analysis.","doi":"10.1348/000711001159366"}
{"fieldsOfStudy":["Medicine","Mathematics"],"year":2009,"outCitations":["44d237af710300110a64fdf96bade1f1604ad8b5","7e6ec081f025f9bdf22f191859031629290dca84","e7f9254f8e9678dedd8c2ccee99624c271ceeb14","dc7c344fb3d0e7dbb702312fb710e7e00c5d79c7","b14f4269f09ef77ad310581ea82d502402d90fb9","370b92bc4f61fedfa64e1b50e7a10c7a6dde0a19","471ea88733f511fd35e44b8f36e5f4c11b01f8b8","64596abdf95560ac4be87c71424d011eb1e51f78"],"journalName":"Journal of biopharmaceutical statistics","paperAbstract":"A weighted least squares statistic is commonly used to test homogeneity of the risk difference for a series of 2 x 2 tables. Since the method is based on asymptotic theory, its type I error rate is inflated when the data are sparse. Two new methods for testing the homogeneity of risk difference across different groups in clinical trials are proposed in this paper. These methods are constructed, based on the Wilson's score test and traditional weighted least squares statistics. The performance of the new methods is evaluated and compared to the currently available approaches. Results show that one of our new methods has a type I error rate that is closest to the nominal level among all the methods and is much more powerful than those proposed by Lipsitz et al.","inCitations":["18460978edad3a469f3414491b9fcfec14f494fd","fa7b5372cb82a8507d26f42208c1fbbdcb60fe87","4243cded47b7b0d4a39f02940420d91b60ce187e"],"title":"Test homogeneity of risk difference across subgroups in clinical trials.","doi":"10.1080/10543400802527874"}
{"fieldsOfStudy":["Medicine","Mathematics"],"year":2015,"outCitations":[],"journalName":"Biostatistics","paperAbstract":"The recent growth of high-throughput transcriptome technology has been paralleled by the development of statistical methodologies to analyze the data they produce. Some of these newly developed methods are based on the assumption that the data observed or a transformation of the data are relatively symmetric with light tails, usually summarized by assuming a Gaussian random component. It is indeed very difficult to assess this assumption for small sample sizes. In this article, we utilize L-moments statistics as the basis of exploratory data analysis, the assessment of distributional assumptions, and the hypothesis testing of high-throughput transcriptomic data. In particular, we use L-moments ratios for assessing the shape (skewness and kurtosis) of high-throughput transcriptome data. Based on these statistics, we propose an algorithm for identifying genes with distributions that are markedly different from the majority in the data. In addition, we also illustrate the utility of this framework to characterize the robustness of distributional assumptions. We apply it to RNA-seq data and find that methods based on the simple [Formula: see text]-test for differential expression analysis using L-moments as weights are robust.","inCitations":["8f460bc4bb4b9887f9c8a5229014ae5d844a697f"],"title":"Shape analysis of high-throughput transcriptomics experiment data.","doi":"10.1093/biostatistics/kxv018"}
{"fieldsOfStudy":["Mathematics","Medicine"],"year":2019,"outCitations":[],"journalName":"Biostatistics","paperAbstract":"","inCitations":["bed9897cef671a37d564bb137993c03bb0d70c73","65d18e7d1b7006751249cb046aeda6f40b3d1584"],"title":"From development to deployment: dataset shift, causality, and shift-stable models in health AI.","doi":"10.1093/biostatistics/kxz041"}
{"fieldsOfStudy":["Computer Science","Mathematics","Medicine"],"year":2015,"outCitations":["6d0a9360bbc8484bdc635738a4a2f059ed89745e","7ef16f997257c30bd5a36b1dec6a3329afd35c4f","c85ab9e99d95247960c2f19cd97a904c2c9db654","c551c6e002ac4a6633476ffd8d97d90336d6c668","df6356460cfc0c293eb63e06c653ae777c0ce272","ee44540cbffa881162169299e0e79a51dc33ff9c","5d7e514d45141178fc1061bd18f1102d8318a8c3","ee7f867739a0d638997bb0f63b6ad18a38bd5be7","5b2c4084eea63062d1d81f203c3a3c0849ad2cdb","b638657cf00bcd705d968c616e50d36f9c6738e2","d7d67697d7b78a7bc3e43346710f1a1b4b332c44","674bc67c3ee55fc46a81994bc3ec3e440ca0da71","ec665c7316ee32409e35aa864f9649828c062c4d","8ae0cbae42a5fb9b340adaed9ed39569eb96b42d","1a77e19441f3a0e030998fb2d11d9dd774582403","862d4aa15e0dc0b4c5f9824e501e15bc87c2a47c","446be8e0ea822e7f91e93098d90130055bed72ae","250b4f05982b491ad80ba8b986d958eedb69a6be","e5208097b88a4f0d73c28c3c669b17ed8f369c22","8be67abc664ebedba70754fc938f9a9ddc271e9b","a3f132288f4797236098766cb2893b5d3afe69f6","1910eb29c724c900ddb9dafac435e0ac23bc0a05","4412d76b8bd046ee9bb36ff77166266131b91d48","d59780c129598857dd7ba888be017446a1849861","06be0ef0f215ef9b000c7237e3ca454b25f8f69f","25fd7d29fc3c1b2fd07fb0d4390ded585ede3978","83730969c0686b1d185bcca39f9b5743fa53ebc1","af6238a1c6ae858337bb382bdf9dacffd383fc50","d9fe5b4a8e4bf4a7daf60e1d51dac8c1971be746","e76f9b50325d44b06e2b90ac65996afd5a06b4d8","fa97c2238a16e9226f386ecffe22095e3d3d9dff","3af2d34cca85244ead2d4be994ea074a05d43656","ba8f7111cacec9e7eb13f40f9abf369444172962","e22dfe0e786fe16fe9e90ca6cf598353ddd2c9f3"],"journalName":"Biostatistics","paperAbstract":"&NA; Outlier detection for high‐dimensional (HD) data is a popular topic in modern statistical research. However, one source of HD data that has received relatively little attention is functional magnetic resonance images (fMRI), which consists of hundreds of thousands of measurements sampled at hundreds of time points. At a time when the availability of fMRI data is rapidly growing—primarily through large, publicly available grassroots datasets—automated quality control and outlier detection methods are greatly needed. We propose principal components analysis (PCA) leverage and demonstrate how it can be used to identify outlying time points in an fMRI run. Furthermore, PCA leverage is a measure of the influence of each observation on the estimation of principal components, which are often of interest in fMRI data. We also propose an alternative measure, PCA robust distance, which is less sensitive to outliers and has controllable statistical properties. The proposed methods are validated through simulation studies and are shown to be highly accurate. We also conduct a reliability study using resting‐state fMRI data from the Autism Brain Imaging Data Exchange and find that removal of outliers using the proposed methods results in more reliable estimation of subject‐level resting‐state networks using independent components analysis.","inCitations":["d27d405a15dd5358de8d4cc244be4262de482d09","25ad22330fe2089ca87a39a32d8e3f5efacf9fc6","a144bc35abd7964434f6ef5e0f617fd331ae9cc8","2dd74c8db02caf1fe8e6ede6957f1c3c10e522cc"],"title":"PCA leverage: outlier detection for high‐dimensional functional magnetic resonance imaging data","doi":"10.1093/biostatistics/kxw050"}
{"fieldsOfStudy":["Computer Science","Mathematics","Medicine"],"year":2014,"outCitations":[],"journalName":"Journal of multivariate analysis","paperAbstract":"Linear mixed models (LMMs) are widely used for regression analysis of data that are assumed to be clustered or correlated. Assessing model fit is important for valid inference but to date no confirmatory tests are available to assess the adequacy of the fixed effects part of LMMs against general alternatives. We therefore propose a class of goodness-of-fit tests for the mean structure of LMMs. Our test statistic is a quadratic form of the difference between observed values and the values expected under the estimated model in cells defined by a partition of the covariate space. We show that this test statistic has an asymptotic chi-squared distribution when model parameters are estimated by maximum likelihood or by least squares and method of moments, and study its power under local alternatives both analytically and in simulations. Data on repeated measurements of thyroglobulin from individuals exposed to the accident at the Chernobyl power plant in 1986 are used to illustrate the proposed test.","inCitations":["b3d02dead09c9a1e4dbbcc8b6abfb1f254e567f2","5760f4c480655c79af673b7dfa995d7d08d935ac","593b4d02e4ce966e0f0492b255774865148bf55a","c41ad0323cb7d0c9d3a44773b230dd67bc0e35ab","3a5455bb83ce4394f02c38f7ab4aa100f40118d1","08a2799b88199901df916108250f8ba5f8a83168","fa87f2d2f744ea58c951769330478ca68e9fa5c6","094cd6f5be6ccaa89a643034d8412b725b5d3f07"],"title":"Goodness of fit tests for linear mixed models","doi":"10.1016/j.jmva.2014.03.012"}
